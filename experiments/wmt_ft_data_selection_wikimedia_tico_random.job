#!/bin/bash

#SBATCH --partition=gpu_a100
#SBATCH --gres=gpu:1
#SBATCH --gpus=1
#SBATCH --job-name=wmt_ft_data_selection_wikimedia_tico_random
#SBATCH --ntasks=1
#SBATCH --time=04:00:00
#SBATCH --output=slurm/slurm_%A.out

module purge
module load 2024
module load Miniconda3/24.7.1-0

# Activate your environment
source activate nlp2
# Run your code
cd ../src

python -m train_data_selection --train_dataset=../data/opus_wikimedia_en_ru --dev_dataset=sethjsa/tico_en_ru --train_from_disk --output_dir /scratch-shared/scur2176/results/wmt_ft_data_selection_wikimedia_tico_random --warmup_steps=100 --model_name=facebook/wmt19-en-ru --reversed_model_name=facebook/wmt19-ru-en --num_train_epochs=5 --dev_sample_percentage 0.05 --save_percentage 0.01 --selection_method=random
python -m evaluate_model --model_name=/scratch-shared/scur2176/results/wmt_ft_data_selection_wikimedia_tico_random --reversed_model_name=facebook/wmt19-ru-en --test_dataset=sethjsa/wmt20bio_en_ru_sent