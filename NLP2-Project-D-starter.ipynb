{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuX1gxFbh1P-"
   },
   "source": [
    "# NLP 2 Project: Backtranslation for Domain Adaptation\n",
    "\n",
    "In this project, you will fine-tune a translation model by backtranslating monolingual in-domain text. You will then test performance in that domain as well as general domains.\n",
    "\n",
    "Your first task is to compare fine-tuning with backtranslation.\n",
    "Next, you will explore a method of data selection.\n",
    "Third, you will extend backtranslation, either modifying decoding, the model, or using multilingual pivots.\n",
    "Finally, you will explore your own research question.\n",
    "\n",
    "This notebook provides starter code to preprocess, fine-tune, and generate with a translation model. This is enough to get you started on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhWYEgH46bWt"
   },
   "outputs": [],
   "source": [
    "# set up environment\n",
    "# if using colab, mount google drive:\n",
    "# from google.colab import drive\n",
    "# import os, sys\n",
    "# drive.mount('/content/drive/')\n",
    "# nb_path = '/content/notebooks'\n",
    "# os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
    "# sys.path.insert(0,nb_path)\n",
    "# !pip install --target=$nb_path ...\n",
    "\n",
    "!pip install transformers==4.49 datasets evaluate torch sacremoses sacrebleu unbabel-comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgECGNQb6mbB"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "# import vllm\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDnACDAIjMay"
   },
   "source": [
    "## Preprocessing\n",
    "First, we need to tokenize our inputs. With HF Transformers, this is fairly simple and is done for you below. Here, we use the model's tokenizer to split the inputs into the model's pre-defined numerical tokens, i.e. convert text into tensors. We also need a function to convert back from tensors into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dINAoOv7aOd"
   },
   "outputs": [],
   "source": [
    "# function to tokenize dataset for translation\n",
    "\n",
    "def preprocess_data(dataset_dict, tokenizer, src_lang, tgt_lang, split, max_length=128):\n",
    "    \"\"\"\n",
    "    Preprocess translation datasets\n",
    "\n",
    "    Args:\n",
    "        dataset_dict: Dictionary containing train/dev/test datasets\n",
    "        tokenizer: Tokenizer object\n",
    "        src_lang: Source language code\n",
    "        tgt_lang: Target language code\n",
    "        split: Dataset split to preprocess ('train', 'validation', etc)\n",
    "        max_length: Maximum sequence length\n",
    "    Returns:\n",
    "        tokenized_dataset: Preprocessed dataset for specified split\n",
    "    \"\"\"\n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[src_lang]\n",
    "        targets = examples[tgt_lang]\n",
    "\n",
    "        model_inputs = tokenizer(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_dataset = dataset_dict[split].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset_dict[split].column_names\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "def postprocess_predictions(predictions, labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert model outputs to decoded text\n",
    "\n",
    "    Args:\n",
    "        predictions: Model predictions\n",
    "        labels: Ground truth labels\n",
    "        tokenizer: Tokenizer object\n",
    "    Returns:\n",
    "        decoded_preds: Decoded predictions\n",
    "        decoded_labels: Decoded labels\n",
    "    \"\"\"\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    return decoded_preds, decoded_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_kEixCZjyL-"
   },
   "source": [
    "## Evaluation\n",
    "During fine-tuning, we need to see how good the outputs are on our dev set. For this, we can use BLEU score (Papineni 2002). This function decodes the predicted tensor tokens, and computes the BLEU score.\n",
    "\n",
    "On our test sets, we also want to calculate an automatic metric, but on decoded text. We can use BLEU again, but also more advanced metrics like COMET. It's up to you to implement your choice of metric. We will discuss some metrics from the literature in class. It's always good to use at least 2 metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4bLB8zShavU"
   },
   "outputs": [],
   "source": [
    "# evaluation: for validation (with raw outputs) and testing (from text)\n",
    "\n",
    "def compute_metrics_val(tokenizer, eval_preds):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for predictions\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Tokenizer object\n",
    "        eval_preds: Tuple of predictions and labels\n",
    "    Returns:\n",
    "        metrics: Dictionary containing BLEU score\n",
    "    \"\"\"\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds, decoded_labels = postprocess_predictions(preds, labels, tokenizer)\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu = load(\"sacrebleu\")\n",
    "    results = bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
    "\n",
    "    return {\"bleu\": results[\"score\"]}\n",
    "\n",
    "def compute_metrics_test(src, tgt, preds, bleu=True, comet=False):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for predictions\n",
    "\n",
    "    Args:\n",
    "        src: Source language texts\n",
    "        tgt: Target language texts\n",
    "        preds: Predicted texts\n",
    "        bleu: Whether to calculate BLEU score\n",
    "        comet: Whether to calculate COMET score\n",
    "    Returns:\n",
    "        metrics: Dictionary containing BLEU score\n",
    "    \"\"\"\n",
    "    if bleu:\n",
    "        bleu = load(\"sacrebleu\")\n",
    "        results = bleu.compute(predictions=preds, references=[[l] for l in tgt])\n",
    "        score = results[\"score\"]\n",
    "    if comet:\n",
    "      raise NotImplementedError(\"COMET not implemented yet\")\n",
    "        # Calculate COMET score\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xc8HvRkkogB"
   },
   "source": [
    "## Fine-tuning\n",
    "Now that we've tokenized our data and got our evaluation ready, we can start fine-tuning (i.e., training from a pre-trained model). This is a minimal training loop.\n",
    "\n",
    "We also need to generate at test time from a text dataset. This function involves generation without calculating gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00mA-hNVhj-Q"
   },
   "outputs": [],
   "source": [
    "# basic training loop\n",
    "\n",
    "def train_model(model_name, tokenized_datasets, tokenizer, training_args):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    # Verify GPU usage\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"WARNING: No GPU detected! Training will be slow.\")\n",
    "    else:\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"dev\"] if \"dev\" in tokenized_datasets else None,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "        compute_metrics=lambda x: compute_metrics_val(tokenizer, x)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model\n",
    "\n",
    "# generation (on GPU) for test time\n",
    "def translate_text(texts, model, tokenizer, max_length=128, batch_size=32):\n",
    "    \"\"\"\n",
    "    Translate texts using the model\n",
    "\n",
    "    Args:\n",
    "        texts: List of texts to translate\n",
    "        model: Translation model\n",
    "        tokenizer: Tokenizer object\n",
    "        max_length: Maximum sequence length\n",
    "        batch_size: Batch size for translation\n",
    "    Returns:\n",
    "        translations: List of translated texts\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    translations = []\n",
    "\n",
    "    # Create tqdm progress bar\n",
    "    progress_bar = tqdm(range(0, len(texts), batch_size), desc=\"Translating\")\n",
    "\n",
    "    for i in progress_bar:\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                temperature=0.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        batch_translations = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        translations.extend(batch_translations)\n",
    "\n",
    "    return translations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwUNfHgylIcR"
   },
   "source": [
    "## Final Setup\n",
    "We now have all the ingredients to run our experiments. This is all standard training code; the interesting results come from what you do with the data. Below, we give an initial setup for getting the code running (either in Colab or on Snellius)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUi-Xho5hwWc"
   },
   "outputs": [],
   "source": [
    "\n",
    "SRC_LANG = \"en\"\n",
    "TGT_LANG = \"ru\"\n",
    "MODEL_NAME = f\"facebook/wmt19-{SRC_LANG}-{TGT_LANG}\"\n",
    "TRAIN_DATASET_NAME = \"sethjsa/flores_en_ru\"\n",
    "DEV_DATASET_NAME = \"sethjsa/tico_en_ru\"\n",
    "TEST_DATASET_NAME = \"sethjsa/tico_en_ru\"\n",
    "OUTPUT_DIR = \"./results\"\n",
    "\n",
    "train_dataset = load_dataset(TRAIN_DATASET_NAME)\n",
    "dev_dataset = load_dataset(DEV_DATASET_NAME)\n",
    "test_dataset = load_dataset(TEST_DATASET_NAME)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# change the splits for actual training. here, using flores-dev as training set because it's small (<1k examples)\n",
    "tokenized_train_dataset = preprocess_data(train_dataset, tokenizer, SRC_LANG, TGT_LANG, \"dev\")\n",
    "tokenized_dev_dataset = preprocess_data(dev_dataset, tokenizer, SRC_LANG, TGT_LANG, \"dev\")\n",
    "tokenized_test_dataset = preprocess_data(test_dataset, tokenizer, SRC_LANG, TGT_LANG, \"test\")\n",
    "\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": tokenized_train_dataset,\n",
    "    \"dev\": tokenized_dev_dataset,\n",
    "    \"test\": tokenized_test_dataset\n",
    "})\n",
    "\n",
    "# modify these as you wish; RQ3 could involve testing effects of various hyperparameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    torch_compile=True, # generally speeds up training, try without it to see if it's faster for small datasets\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32, # change batch sizes to fit your GPU memory and train faster\n",
    "    per_device_eval_batch_size=128,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    save_total_limit=1, # modify this to save more checkpoints\n",
    "    num_train_epochs=1, # modify this to train more epochs\n",
    "    predict_with_generate=True,\n",
    "    generation_num_beams=4,\n",
    "    generation_max_length=128,\n",
    "    no_cuda=False,  # Set to False to enable GPU\n",
    "    fp16=True,      # Enable mixed precision training for faster training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lfvlbh0lory"
   },
   "outputs": [],
   "source": [
    "# fine-tune model\n",
    "model = train_model(MODEL_NAME, tokenized_datasets, tokenizer, training_args)\n",
    "\n",
    "# test model\n",
    "predictions = translate_text(test_dataset[\"test\"][SRC_LANG], model, tokenizer, max_length=128, batch_size=64)\n",
    "print(predictions)\n",
    "\n",
    "eval_score = compute_metrics_test(test_dataset[\"test\"][SRC_LANG], test_dataset[\"test\"][TGT_LANG], predictions, bleu=False, comet=True)\n",
    "print(eval_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8U1vIgwmE-1"
   },
   "source": [
    "You will find all the datasets for this project under: https://huggingface.co/sethjsa\n",
    "\n",
    "For other models, consider \"Helsinki-NLP/opus-mt-en-ru\" (general MT model), \"glazzova/translation_en_ru\" (tuned on biomedical domain), or \"facebook/m2m100_418M\" (multilingual model with 100 languages -- consider using for multilingual pivot experiments).\n",
    "\n",
    "To read more about the WMT Biomedical test data, see here: https://aclanthology.org/2022.wmt-1.69/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adDwo_fur05B"
   },
   "source": [
    "## Snellius advice\n",
    "\n",
    "If using snellius, I recommend converting the above into a training script. Then you can submit jobs with slurm. See advice here:\n",
    "https://servicedesk.surf.nl/wiki/spaces/WIKI/pages/30660217/Creating+and+running+jobs\n",
    "\n",
    "https://servicedesk.surf.nl/wiki/spaces/WIKI/pages/30660220/Writing+a+job+script\n",
    "\n",
    "https://servicedesk.surf.nl/wiki/spaces/WIKI/pages/30660228/Interacting+with+the+job+queue\n",
    "\n",
    "https://servicedesk.surf.nl/wiki/spaces/WIKI/pages/30660234/Example+job+scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-jzQG5Vlh2X"
   },
   "source": [
    "# Advanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plzMI5gBmW_5"
   },
   "source": [
    "\n",
    "ONLY if you have GPU hours left and want to generate backtranslations with an LLM, consider using vLLM for faster generation. An example function is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGqrOvTrhsms"
   },
   "outputs": [],
   "source": [
    "\n",
    "# if using LLM for generation, consider using vllm for faster generation\n",
    "def translate_text_vllm(texts, model_name, tokenizer, max_length=128, batch_size=32):\n",
    "    \"\"\"\n",
    "    Translate texts using vllm for faster generation\n",
    "\n",
    "    Args:\n",
    "        texts: List of texts to translate\n",
    "        model_name: Name or path of the model (str)\n",
    "        tokenizer: Tokenizer object\n",
    "        max_length: Maximum sequence length\n",
    "        batch_size: Batch size for translation\n",
    "    Returns:\n",
    "        translations: List of translated texts\n",
    "    \"\"\"\n",
    "    # Use model_name instead of model object\n",
    "    llm = vllm.LLM(\n",
    "        model=model_name,  # Changed from model to model_name\n",
    "        tokenizer=tokenizer,\n",
    "        tensor_parallel_size=1,\n",
    "        max_num_batched_tokens=max_length * batch_size\n",
    "    )\n",
    "\n",
    "    # Create sampling params\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        temperature=0.0,  # Equivalent to greedy decoding\n",
    "        max_tokens=max_length,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    # Generate translations in batches\n",
    "    translations = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        outputs = llm.generate(batch, sampling_params)\n",
    "\n",
    "        # Extract generated text from outputs\n",
    "        batch_translations = [output.outputs[0].text for output in outputs]\n",
    "        translations.extend(batch_translations)\n",
    "\n",
    "    return translations"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjAsnxOtpCzePND80zC+ew",
   "collapsed_sections": [
    "6-jzQG5Vlh2X"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
